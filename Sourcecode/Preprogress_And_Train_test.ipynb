{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install flwr tensorflow numpy pandas scikit-learn\n",
        "!pip install -q flwr[simulation] tensorflow\n",
        "!pip install ray[rllib]  # also recommended: ray[debug]"
      ],
      "metadata": {
        "id": "iBQiODEiuHL3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8c42e4-314d-4c80-bc91-e9097b9288dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flwr\n",
            "  Downloading flwr-1.13.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Collecting cryptography<43.0.0,>=42.0.4 (from flwr)\n",
            "  Downloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting grpcio!=1.64.2,<2.0.0,<=1.64.3,>=1.60.0 (from flwr)\n",
            "  Downloading grpcio-1.64.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting iterators<0.0.3,>=0.0.2 (from flwr)\n",
            "  Downloading iterators-0.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pathspec<0.13.0,>=0.12.1 (from flwr)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.25.2 in /usr/local/lib/python3.10/dist-packages (from flwr) (4.25.5)\n",
            "Collecting pycryptodome<4.0.0,>=3.18.0 (from flwr)\n",
            "  Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.10/dist-packages (from flwr) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from flwr) (2.2.1)\n",
            "Collecting tomli-w<2.0.0,>=1.0.0 (from flwr)\n",
            "  Downloading tomli_w-1.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting typer<0.13.0,>=0.12.5 (from flwr)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<43.0.0,>=42.0.4->flwr) (1.17.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (2.18.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<43.0.0,>=42.0.4->flwr) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Downloading flwr-1.13.1-py3-none-any.whl (512 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.2/512.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.64.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iterators-0.0.2-py3-none-any.whl (3.9 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.1.0-py3-none-any.whl (6.4 kB)\n",
            "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tomli-w, pycryptodome, pathspec, iterators, grpcio, cryptography, typer, flwr\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.68.1\n",
            "    Uninstalling grpcio-1.68.1:\n",
            "      Successfully uninstalled grpcio-1.68.1\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.3\n",
            "    Uninstalling cryptography-43.0.3:\n",
            "      Successfully uninstalled cryptography-43.0.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.14.0\n",
            "    Uninstalling typer-0.14.0:\n",
            "      Successfully uninstalled typer-0.14.0\n",
            "Successfully installed cryptography-42.0.8 flwr-1.13.1 grpcio-1.64.3 iterators-0.0.2 pathspec-0.12.1 pycryptodome-3.21.0 tomli-w-1.1.0 typer-0.12.5\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ray[rllib] in /usr/local/lib/python3.10/dist-packages (2.10.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (3.16.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (24.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (4.25.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.2.2)\n",
            "Collecting tensorboardX>=1.9 (from ray[rllib])\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (17.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2024.10.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.1.8)\n",
            "Collecting gymnasium==0.28.1 (from ray[rllib])\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting lz4 (from ray[rllib])\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.24.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.13.1)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.12.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (13.9.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (1.26.4)\n",
            "Collecting jax-jumpy>=1.0.0 (from gymnasium==0.28.1->ray[rllib])\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium==0.28.1->ray[rllib])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (0.22.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]) (2.18.0)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (3.4.2)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (11.0.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (0.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->ray[rllib]) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->ray[rllib]) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ray[rllib]) (1.16.0)\n",
            "Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: farama-notifications, tensorboardX, lz4, jax-jumpy, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0 lz4-4.3.3 tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect Driver\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9sASgORbN3U",
        "outputId": "b6e8b2e1-a2bf-434b-a0e8-73a37ac1d05a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MLp9aNzWazp9"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import ray\n",
        "from pyarrow import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import flwr as fl\n",
        "from flwr.common import Metrics\n",
        "from flwr.simulation.ray_transport.utils import enable_tf_gpu_growth\n",
        "import math\n",
        "from typing import Dict, List, Tuple\n",
        "import glob\n",
        "from flwr.simulation import run_simulation\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc, average_precision_score, f1_score\n",
        "from tensorflow.keras.models import load_model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import sklearn\n",
        "import io\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D, GRU\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, Normalizer\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Features - UNSWNB-15"
      ],
      "metadata": {
        "id": "WbOEKnKj4q9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_HEADER = [\n",
        "    \"dur\", \"proto\", \"service\", \"state\", \"spkts\", \"dpkts\", \"sbytes\", \"dbytes\", \"rate\", \"sttl\", \"dttl\",\n",
        "    \"sload\", \"dload\", \"sloss\", \"dloss\", \"sinpkt\", \"dinpkt\", \"sjit\", \"djit\", \"swin\", \"stcpb\", \"dtcpb\",\n",
        "    \"dwin\", \"tcprtt\", \"synack\", \"ackdat\", \"smean\", \"dmean\", \"trans_depth\", \"response_body_len\",\n",
        "    \"ct_srv_src\", \"ct_state_ttl\", \"ct_dst_ltm\", \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\",\n",
        "    \"ct_dst_src_ltm\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_flw_http_mthd\", \"ct_src_ltm\",\n",
        "    \"ct_srv_dst\",\"is_sm_ips_ports\",\"label\"\n",
        "] # drop attack_cat,\n",
        "CSV_HEADER_WITHOUT_LABEL = [\n",
        "    \"dur\", \"proto\", \"service\", \"state\", \"spkts\", \"dpkts\", \"sbytes\", \"dbytes\", \"rate\", \"sttl\", \"dttl\",\n",
        "    \"sload\", \"dload\", \"sloss\", \"dloss\", \"sinpkt\", \"dinpkt\", \"sjit\", \"djit\", \"swin\", \"stcpb\", \"dtcpb\",\n",
        "    \"dwin\", \"tcprtt\", \"synack\", \"ackdat\", \"smean\", \"dmean\", \"trans_depth\", \"response_body_len\",\n",
        "    \"ct_srv_src\", \"ct_state_ttl\", \"ct_dst_ltm\", \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\",\n",
        "    \"ct_dst_src_ltm\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_flw_http_mthd\", \"ct_src_ltm\",\n",
        "    \"ct_srv_dst\",\"is_sm_ips_ports\"\n",
        "] # drop attack_cat,"
      ],
      "metadata": {
        "id": "ehn-KMY7dB8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#UNSWNB-15"
      ],
      "metadata": {
        "id": "1i__SQJRR33n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(path_train,path_test): #CSV_HEADER\n",
        "  # train_data = pd.read_csv(path_train, header=None, names=CSV_HEADER)\n",
        "  # test_data = pd.read_csv(path_test, header=None, names=CSV_HEADER)\n",
        "  train_data = pd.read_csv(path_train)\n",
        "  test_data = pd.read_csv(path_test)\n",
        "  train_data = train_data.drop(['attack_cat'], axis=1)\n",
        "  test_data = test_data.drop(['attack_cat'], axis=1)\n",
        "  train_data = train_data.drop(['id'], axis=1)\n",
        "  test_data = test_data.drop(['id'], axis=1)\n",
        "  print(f\"Train dataset shape: {train_data.shape}\")\n",
        "  print(f\"Test dataset shape: {test_data.shape}\")\n",
        "  return train_data, test_data\n",
        "#/content/drive/MyDrive/BigDataAgain/Dataset/NSL-KDD/NSL_KDD_Train.csv\n",
        "#/content/drive/MyDrive/BigDataAgain/Dataset/NSL-KDD/NSL_KDD_Test.csv"
      ],
      "metadata": {
        "id": "zF7mApIhbXt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_categorical_features(train_data,test_data):\n",
        "  train_data['service']=train_data['service'].apply(lambda x: 'none' if x=='-' else x )\n",
        "  test_data['service']=test_data['service'].apply(lambda x: 'none' if x=='-' else x )\n",
        "  train_data['state']=train_data['state'].apply(lambda x: 'none' if x=='-' else x )\n",
        "  test_data['state']=test_data['state'].apply(lambda x: 'none' if x=='-' else x )\n",
        "  CATEGORICAL_FEATURE_NAMES = [\"proto\",\"service\",\"state\"]\n",
        "  # Danh sách các cột cần Label Encode\n",
        "  CATEGORICAL_FEATURE_NAMES = [\"proto\",\"service\",\"state\"]\n",
        "\n",
        "  # Label Encoding cho từng cột\n",
        "  label_encoders = {}\n",
        "  for col in CATEGORICAL_FEATURE_NAMES:\n",
        "      label_encoder = LabelEncoder()\n",
        "      train_data[col] = label_encoder.fit_transform(train_data[col])\n",
        "      test_data[col] = label_encoder.fit_transform(test_data[col])\n",
        "      label_encoders[col] = label_encoder  # Lưu lại encoder nếu cần inverse_transform sau này\n",
        "  return train_data,test_data"
      ],
      "metadata": {
        "id": "xjNvC0iidbZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_numeric_features(train_data,test_data):\n",
        "  #Preprocessing Numerical Features.\n",
        "  scaler_train = StandardScaler()\n",
        "  scaler_train = scaler_train.fit(train_data[CSV_HEADER_WITHOUT_LABEL])\n",
        "  train_data[CSV_HEADER_WITHOUT_LABEL] = scaler_train.transform(train_data[CSV_HEADER_WITHOUT_LABEL])\n",
        "\n",
        "  scaler_test = StandardScaler()\n",
        "  scaler_test = scaler_test.fit(test_data[CSV_HEADER_WITHOUT_LABEL])\n",
        "  test_data[CSV_HEADER_WITHOUT_LABEL] = scaler_test.transform(test_data[CSV_HEADER_WITHOUT_LABEL])\n",
        "  return train_data,test_data"
      ],
      "metadata": {
        "id": "98eR47CPbYOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exchange_label(train_data,test_data):\n",
        "\n",
        "  #Convert \"label\" into normal=0 and attack=1 for KDDTrain+\n",
        "  train_data['label']=train_data['label'].apply(lambda x: 'ibnormal' if x==1 else 'normal')\n",
        "  #train_data.drop(['label'], axis=1)\n",
        "\n",
        "  #Convert \"label\" into normal=0 and attack=1 for KDDTest+\n",
        "  test_data['label']=test_data['label'].apply(lambda x: 'ibnormal' if x==1 else 'normal')\n",
        "  #test_data.drop(['label'],1)\n",
        "\n",
        "  train_data = train_data.fillna(axis=0, method='ffill')\n",
        "  test_data = test_data.fillna(axis=0,method='ffill')\n",
        "  return train_data, test_data"
      ],
      "metadata": {
        "id": "rNxpTSEDbZs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_file(train_data,test_data):\n",
        "  train_data_file = \"train_data.csv\"\n",
        "  test_data_file = \"test_data.csv\"\n",
        "\n",
        "  train_data.to_csv(train_data_file, index=False, header=False)\n",
        "  test_data.to_csv(test_data_file, index=False, header=False)\n",
        "  return train_data_file, test_data_file"
      ],
      "metadata": {
        "id": "jZ0NfogpbdZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data,test_data = read_data(\"/content/drive/MyDrive/AnToanHeThongThongTin-CH2024/UNSW_NB15_training-set.csv\",\"/content/drive/MyDrive/AnToanHeThongThongTin-CH2024/UNSW_NB15_testing-set.csv\")\n",
        "train_data,test_data = preprocess_categorical_features(train_data,test_data)\n",
        "train_data,test_data = preprocess_numeric_features(train_data,test_data)\n",
        "#train_data, test_data = exchange_label(train_data,test_data)\n",
        "train_data_1 ,train_data_2 = train_test_split(train_data,test_size=0.5)\n",
        "train_data_1.to_csv(\"train_data_1.csv\", index=False, header=True)\n",
        "train_data_2.to_csv(\"train_data_2.csv\", index=False, header=True)\n",
        "test_data.to_csv(\"test_data.csv\", index=False, header=True)\n",
        "#train_data_file,test_data_file = save_file(train_data,test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCQ1xqe8lPbj",
        "outputId": "10767951-c51f-482a-ba14-20b5b840b5ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset shape: (175341, 43)\n",
            "Test dataset shape: (82332, 43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Features - CIC_UNSW-NB15"
      ],
      "metadata": {
        "id": "ehvGamNj4vHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_HEADER=['Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
        "       'Total Length of Fwd Packet', 'Total Length of Bwd Packet',\n",
        "       'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
        "       'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
        "       'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
        "       'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s',\n",
        "       'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max',\n",
        "       'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std',\n",
        "       'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean',\n",
        "       'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags',\n",
        "       'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length',\n",
        "       'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
        "       'Packet Length Min', 'Packet Length Max', 'Packet Length Mean',\n",
        "       'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n",
        "       'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
        "       'URG Flag Count', 'CWR Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
        "       'Average Packet Size', 'Fwd Segment Size Avg', 'Bwd Segment Size Avg',\n",
        "       'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg',\n",
        "       'Bwd Bytes/Bulk Avg', 'Bwd Packet/Bulk Avg', 'Bwd Bulk Rate Avg',\n",
        "       'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets',\n",
        "       'Subflow Bwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes',\n",
        "       'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std',\n",
        "       'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max',\n",
        "       'Idle Min', 'Label', 'attack_cat'] # bo Src_IP, Dst_IP, Timestamp, {'Flow ID', 'Src Port', 'Dst Port', 'Protocol'}\n",
        "CSV_HEADER_WITHOUT_LABEL = ['Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
        "       'Total Length of Fwd Packet', 'Total Length of Bwd Packet',\n",
        "       'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
        "       'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
        "       'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
        "       'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s',\n",
        "       'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max',\n",
        "       'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std',\n",
        "       'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean',\n",
        "       'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags',\n",
        "       'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length',\n",
        "       'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
        "       'Packet Length Min', 'Packet Length Max', 'Packet Length Mean',\n",
        "       'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n",
        "       'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
        "       'URG Flag Count', 'CWR Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
        "       'Average Packet Size', 'Fwd Segment Size Avg', 'Bwd Segment Size Avg',\n",
        "       'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg',\n",
        "       'Bwd Bytes/Bulk Avg', 'Bwd Packet/Bulk Avg', 'Bwd Bulk Rate Avg',\n",
        "       'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets',\n",
        "       'Subflow Bwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes',\n",
        "       'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std',\n",
        "       'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max',\n",
        "       'Idle Min']\n",
        "\n",
        "# Bảng mã hóa\n",
        "encoded_to_original = {\n",
        "    0: 'Benign',\n",
        "    1: 'Analysis',\n",
        "    2: 'Backdoor',\n",
        "    3: 'DoS',\n",
        "    4: 'Exploits',\n",
        "    5: 'Fuzzers',\n",
        "    6: 'Generic',\n",
        "    7: 'Reconnaissance',\n",
        "    8: 'Shellcode',\n",
        "    9: 'Worms'\n",
        "}\n",
        "# decoded_labels = [encoded_to_original[label] for label in y_pred]\n",
        "# decoded_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1-xXx8G425d",
        "outputId": "4703d159-d9c0-4127-f7d7-f3e7b852bf54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CIC_UNSW_NB15"
      ],
      "metadata": {
        "id": "cpabI1YcRyEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/CIC_UNSW-LABEL_Attack_cat.csv\")\n",
        "df = df.drop(['attack_cat'], axis=1)\n",
        "scaler = StandardScaler()\n",
        "scaler = scaler.fit(df[CSV_HEADER_WITHOUT_LABEL])\n",
        "df[CSV_HEADER_WITHOUT_LABEL] = scaler.transform(df[CSV_HEADER_WITHOUT_LABEL])"
      ],
      "metadata": {
        "id": "8bllg777CfkX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chia data_test\n",
        "train_data,test_data = train_test_split(df,test_size=0.2)\n",
        "train_data_1 ,train_data_2 = train_test_split(train_data,test_size=0.5)\n",
        "train_data_1.to_csv(\"train_data_1.csv\", index=False, header=True)\n",
        "train_data_2.to_csv(\"train_data_2.csv\", index=False, header=True)\n",
        "train_data.to_csv(\"train_data.csv\", index=False, header=True)\n",
        "test_data.to_csv(\"test_data.csv\", index=False, header=True)"
      ],
      "metadata": {
        "id": "Kq-T0vo_DDBx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Client and Server for Distributed"
      ],
      "metadata": {
        "id": "H58UQ48XnjMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, x_train, y_train,model) -> None:\n",
        "        # Create model\n",
        "        self.model = model\n",
        "        self.x_train, self.y_train = x_train, y_train\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return self.model.get_weights()\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "\n",
        "        self.model.set_weights(parameters)\n",
        "        self.model.fit(\n",
        "             self.x_train,  self.y_train, epochs=3, batch_size=32, verbose=VERBOSE\n",
        "        )\n",
        "        return self.model.get_weights(), len(self.x_train), {}"
      ],
      "metadata": {
        "id": "YFdE5xQtnvKr"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_client_fn(dataset_partitions,model):\n",
        "    \"\"\"Return a function to be executed by the VirtualClientEngine in order to construct\n",
        "    a client.\"\"\"\n",
        "\n",
        "    def client_fn(cid: str) -> fl.client.Client:\n",
        "        \"\"\"Construct a FlowerClient with its own dataset partition.\"\"\"\n",
        "\n",
        "        # Extract partition for client with id = cid\n",
        "        x_train, y_train = dataset_partitions[int(cid)]\n",
        "\n",
        "        # Create and return client\n",
        "        return FlowerClient(x_train, y_train, model)\n",
        "\n",
        "    return client_fn\n",
        "\n",
        "\n",
        "def partition_dataset(x, y, num_clients):\n",
        "    partition_size = math.floor(len(x) / num_clients)\n",
        "    partitions = []\n",
        "    for i in range(num_clients):\n",
        "        idx_from, idx_to = i * partition_size, (i + 1) * partition_size\n",
        "        partitions.append((x[idx_from:idx_to], y[idx_from:idx_to]))\n",
        "    return partitions\n",
        "\n",
        "def get_evaluate_fn(model,file_path):\n",
        "    \"\"\"Return an evaluation function for server-side (i.e. centralized) evaluation.\"\"\"\n",
        "\n",
        "    # The `evaluate` function will be called after every round by the strategy\n",
        "    def evaluate(\n",
        "        server_round: int,\n",
        "        parameters: fl.common.NDArrays,\n",
        "        config: Dict[str, fl.common.Scalar],\n",
        "    ):\n",
        "        model.set_weights(parameters)  # Update model with the latest parameters\n",
        "        model.save(file_path)\n",
        "        # model_report(model,x_test,y_test)\n",
        "        return\n",
        "\n",
        "    return evaluate\n"
      ],
      "metadata": {
        "id": "g4rpvo98nx9H"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_partitions(data_path):\n",
        "    partitions = []\n",
        "\n",
        "    for f in glob.glob(data_path):\n",
        "        data = pd.read_csv(f, header=0)\n",
        "\n",
        "        data.columns = data.columns.str.strip()\n",
        "\n",
        "        # data = data[data[\"Class\"] != ' null']\n",
        "        # data[\"Class\"] = data[\"Class\"].replace({' \"-1\"' : 0 , ' \"Facet\"' : 1 })\n",
        "        # data[\"Class\"] = data[\"Class\"].replace({'-1' : 0, 'Facet' : 1 })\n",
        "        # data[\"Class\"] = data[\"Class\"].replace({' -1' : 0, ' Facet' : 1 })\n",
        "        # data[\"Class\"] = data[\"Class\"].replace(np.nan,'benign')\n",
        "        #data[\"label\"] = np.where(data[\"label\"]=='normal',0,1) # 'normal' -> 0, other ->1\n",
        "        #data = data.sample(frac = 1).reset_index(drop=True)\n",
        "\n",
        "        x_data = data.iloc[:,:-1]\n",
        "        y_data = data[\"Label\"]\n",
        "\n",
        "        partitions.append((x_data.to_numpy(),y_data.to_numpy()))\n",
        "\n",
        "    return partitions"
      ],
      "metadata": {
        "id": "DcBzFSrtoH-L"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_report_list(model, df_test):\n",
        "    result = []\n",
        "\n",
        "    x_test = df_test.iloc[:, :-1]\n",
        "    y_test = df_test[\"label\"]\n",
        "\n",
        "    # Get model predictions for the test data\n",
        "    y_pred_prob = model.predict(x_test, verbose=0)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "    df_test[\"PLabel\"] = y_pred\n",
        "\n",
        "    # # Calculate overall accuracy\n",
        "    # overall_accuracy = accuracy_score(y_test, y_pred)\n",
        "    # result.append(('Overall Accuracy', overall_accuracy))\n",
        "\n",
        "    # Calculate F1 Score\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    result.append(('F1 Score', f1))\n",
        "\n",
        "    # Calculate AUC\n",
        "    auc_score = roc_auc_score(y_test, y_pred_prob)\n",
        "    result.append(('AUC', auc_score))\n",
        "\n",
        "    # # Calculate AUC-PR\n",
        "    # precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
        "    # auc_pr_score = auc(recall, precision)\n",
        "    # result.append(('AUC-PR', auc_pr_score))\n",
        "\n",
        "    # # Calculate and print per-class accuracy\n",
        "    # unique_classes = np.unique(y_test)\n",
        "    # confusion_mat = confusion_matrix(y_test, y_pred, labels=unique_classes)\n",
        "    # per_class_accuracy = np.diag(confusion_mat) / np.sum(confusion_mat, axis=1)\n",
        "    # for class_id, accuracy in zip(unique_classes, per_class_accuracy):\n",
        "    #     result.append((f'Accuracy Class {class_id}', accuracy))\n",
        "\n",
        "    # # Calculate False Positives and False Negatives\n",
        "    # FP = confusion_mat[0][1]\n",
        "    # FN = confusion_mat[1][0]\n",
        "    # result.append(('False Positives', FP))\n",
        "    # result.append(('False Negatives', FN))\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "kh2cERrkwGH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CREATE MODEL"
      ],
      "metadata": {
        "id": "BHwbRoBDnQYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# ANN Model Creationg Function\n",
        "def create_ann_model(input_shape=(42,), hidden_units=None):\n",
        "    \"\"\"\n",
        "    Create a feedforward neural network model for binary classification with\n",
        "    the specified input shape.\n",
        "\n",
        "    Parameters:\n",
        "    - input_shape: Tuple specifying the shape of the input data.\n",
        "    - hidden_units: List of integers specifying the number of units in each hidden layer.\n",
        "                    Default is [64, 32].\n",
        "\n",
        "    Returns:\n",
        "    - A compiled Keras model.\n",
        "    \"\"\"\n",
        "\n",
        "    if hidden_units is None:\n",
        "        hidden_units = [64, 32]\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer\n",
        "    model.add(Dense(hidden_units[0], activation='relu', input_shape=input_shape))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Hidden layers\n",
        "    for units in hidden_units[1:]:\n",
        "        model.add(Dense(units, activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "    # Output layer for binary classification\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "64iwkXbopMbM"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xây dựng model cho phân loại"
      ],
      "metadata": {
        "id": "LScQzVdAQZbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ann_model_multiclass(input_shape=(76,), hidden_units=None, num_classes=10):\n",
        "    \"\"\"\n",
        "    Create a feedforward neural network model for multiclass classification with\n",
        "    the specified input shape and number of classes.\n",
        "\n",
        "    Parameters:\n",
        "    - input_shape: Tuple specifying the shape of the input data.\n",
        "    - hidden_units: List of integers specifying the number of units in each hidden layer.\n",
        "                    Default is [64, 32].\n",
        "    - num_classes: Integer specifying the number of output classes (e.g., 10 for multiclass classification).\n",
        "\n",
        "    Returns:\n",
        "    - A compiled Keras model.\n",
        "    \"\"\"\n",
        "    if hidden_units is None:\n",
        "        hidden_units = [64, 32]\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer\n",
        "    model.add(Dense(hidden_units[0], activation='relu', input_shape=input_shape))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Hidden layers\n",
        "    for units in hidden_units[1:]:\n",
        "        model.add(Dense(units, activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "    # Output layer for multiclass classification\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',  # Loss function for integer labels\n",
        "        optimizer='nadam',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "IY5-pP_tFa3x"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train and TEST_model"
      ],
      "metadata": {
        "id": "cFCBXwoSnUwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model without Distributed"
      ],
      "metadata": {
        "id": "Ds269hyXoSrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Đọc dữ liệu từ file CSV không có header\n",
        "train_data = pd.read_csv('/content/train_data.csv', header=None)\n",
        "  # Lấy tất cả các cột trừ cột cuối (label)\n",
        "train_labels = train_data.values[:, -1]  # Lấy cột cuối làm nhãn (label)\n",
        "train_labels = np.where(train_labels == 'normal', 0, 1) # 'normal' -> 0, other ->1\n",
        "train_data = train_data.values[:, :-1].astype(np.float32)\n",
        "truclb_model = create_ann_model()\n",
        "truclb_model.fit(\n",
        "    train_data,         # Dữ liệu đầu vào\n",
        "    train_labels,       # Nhãn đầu ra\n",
        "    epochs=1,          # Số epoch (lần lặp qua dữ liệu)\n",
        "    batch_size=32,      # Kích thước batch\n",
        "    validation_split=0.5  # Chia 20% dữ liệu để validation\n",
        ")\n",
        "truclb_model.save('truclb_model_ver1-0.keras')  # Lưu mô hình dưới dạng SavedModel"
      ],
      "metadata": {
        "id": "480XdNnLphtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test model Without Distributed"
      ],
      "metadata": {
        "id": "5sxkPxJAnXU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Đọc dữ liệu từ file CSV không có header\n",
        "test_data = pd.read_csv('/content/test_data.csv', header=None)\n",
        "  # Lấy tất cả các cột trừ cột cuối (label)\n",
        "test_labels = test_data.values[:, -1]  # Lấy cột cuối làm nhãn (label)\n",
        "test_labels = np.where(test_labels == 'normal', 0, 1) # 'normal' -> 0, other ->1\n",
        "test_data = test_data.values[:, :-1].astype(np.float32)"
      ],
      "metadata": {
        "id": "UprHRN2IqN88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report,ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix , classification_report, recall_score, precision_score,f1_score\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "model_reload = load_model('truclb_model_ver1-0.keras')\n",
        "y_pred = model_reload.predict(test_data)\n",
        "def print_Report(y_true,y_pred_bool):\n",
        "    print(classification_report(y_true, y_pred_bool))\n",
        "    #20/12\n",
        "    #Confusion matrix -> vẽ và phân tích.\n",
        "    print('_______________________CONFUSION MATRIX_______________________')\n",
        "    cm = confusion_matrix(y_true,y_pred_bool)\n",
        "    cmp = ConfusionMatrixDisplay(cm, display_labels=[0, 1])\n",
        "    cmp.plot()\n",
        "    return\n",
        "print_Report(test_labels,K.round(y_pred))"
      ],
      "metadata": {
        "id": "QVI7hwtfupcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train model with Distributed Learning"
      ],
      "metadata": {
        "id": "5Vi5_pZGoPFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Global setting\n"
      ],
      "metadata": {
        "id": "K8JH4e7Onv_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VERBOSE = 0\n",
        "NUM_CLIENTS = 2\n",
        "\n",
        "# With a dictionary, you tell Flower's VirtualClientEngine that each\n",
        "# client needs exclusive access to these many resources in order to run\n",
        "client_resources = {\"num_cpus\":2,\"num_gpus\": 0}"
      ],
      "metadata": {
        "id": "eQWYM3qxn0e3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "aUJ9zZobose6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MedianFedAvg(fl.server.strategy.FedAvg):\n",
        "    def aggregate_fit(self, rnd, results, failures):\n",
        "        \"\"\"\n",
        "        Ghi đè phương thức aggregate_fit để sử dụng Median-based Federated Averaging.\n",
        "        \"\"\"\n",
        "        if not results:\n",
        "            return None, {}\n",
        "\n",
        "        # Lấy trọng số mô hình từ tất cả các client\n",
        "        weights = [fit_res.parameters.tensors for _, fit_res in results]\n",
        "\n",
        "        # Chuyển đổi trọng số từ list sang numpy arrays\n",
        "        client_weights = [[np.array(layer) for layer in model_weights] for model_weights in weights]\n",
        "\n",
        "        # Tính trung vị cho từng lớp trọng số\n",
        "        aggregated_weights = []\n",
        "        for weights_per_layer in zip(*client_weights):  # Lặp qua từng lớp\n",
        "            weight_array = np.array(weights_per_layer)\n",
        "            median_weights = np.median(weight_array, axis=0)\n",
        "            aggregated_weights.append(median_weights)\n",
        "\n",
        "        # Trả về trọng số được tổng hợp dưới dạng `Parameters`\n",
        "        aggregated_parameters = fl.common.ndarrays_to_parameters(aggregated_weights)\n",
        "        return aggregated_parameters, {}"
      ],
      "metadata": {
        "id": "5WULMuaBGlno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flwr.simulation import run_simulation,start_simulation\n",
        "result = []\n",
        "\n",
        "partitions = get_partitions(\"./dataset/*\")\n",
        "\n",
        "for _ in range(2):\n",
        "\n",
        "    hidden_units = [64,32]\n",
        "    output_model = f\"./models/i_medianFedAvg_MLP{_}.keras\"\n",
        "\n",
        "    #model = create_ann_model(hidden_units=hidden_units) #làm cho binary\n",
        "    model = create_ann_model_multiclass(hidden_units=hidden_units) #Làm cho Multiclass\n",
        "    #model = create_MLPClassifier()\n",
        "\n",
        "    strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=1,\n",
        "    # min_fit_clients=10,\n",
        "    fraction_evaluate = 1,\n",
        "    min_available_clients = NUM_CLIENTS,\n",
        "    evaluate_fn = get_evaluate_fn(model,output_model),\n",
        "    )\n",
        "#     strategy = MedianFedAvg(\n",
        "#     fraction_fit=1,\n",
        "#     fraction_evaluate=1,\n",
        "#     min_available_clients=NUM_CLIENTS,\n",
        "#     evaluate_fn=get_evaluate_fn(model, output_model),\n",
        "# )\n",
        "\n",
        "    # Start simulation\n",
        "    history = fl.simulation.start_simulation(\n",
        "    client_fn=get_client_fn(partitions,model),\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=1),\n",
        "    strategy=strategy,\n",
        "    client_resources=client_resources,\n",
        "    # actor_kwargs={\n",
        "    # \"on_actor_init_fn\": enable_tf_gpu_growth  },\n",
        "    )\n",
        "    del history\n",
        "    del strategy\n",
        "    del model\n",
        "\n",
        "    # model = load_model(output_model)\n",
        "    # s = model_report_list(model,test_data.copy())\n",
        "\n",
        "    # result.append(s)\n",
        "    # del model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UHjnE_pPoRtv",
        "outputId": "17fc7efb-c10f-40c0-f824-dc5e3cb81c8e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
            "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
            "\n",
            "\t\t$ flwr new  # Create a new Flower app from a template\n",
            "\n",
            "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
            "\n",
            "\tUsing `start_simulation()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=1, no round_timeout\n",
            "2024-12-06 14:00:18,924\tINFO worker.py:1752 -- Started a local Ray instance.\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'memory': 7910431950.0, 'object_store_memory': 3955215974.0, 'node:__internal_head__': 1.0, 'node:172.28.0.12': 1.0, 'CPU': 2.0}\n",
            "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0}\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 1 actors\n",
            "\u001b[92mINFO \u001b[0m:      [INIT]\n",
            "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
            "\u001b[36m(pid=18958)\u001b[0m 2024-12-06 14:00:22.652156: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=18958)\u001b[0m 2024-12-06 14:00:22.676788: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=18958)\u001b[0m 2024-12-06 14:00:22.683907: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=18958)\u001b[0m 2024-12-06 14:00:24.198539: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
            "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
            "\u001b[91mERROR \u001b[0m:     'MLPClassifier' object has no attribute 'set_weights'\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/legacy_app.py\", line 359, in start_simulation\n",
            "    hist = run_fl(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/server/server.py\", line 492, in run_fl\n",
            "    hist, elapsed_time = server.fit(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/server/server.py\", line 95, in fit\n",
            "    res = self.strategy.evaluate(0, parameters=self.parameters)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/server/strategy/fedavg.py\", line 167, in evaluate\n",
            "    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})\n",
            "  File \"<ipython-input-37-02c4b298bd5d>\", line 34, in evaluate\n",
            "    model.set_weights(parameters)  # Update model with the latest parameters\n",
            "AttributeError: 'MLPClassifier' object has no attribute 'set_weights'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Your simulation crashed :(. This could be because of several reasons. The most common are: \n",
            "\t > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: \n",
            "\t\t - You might be using a class attribute in your clients that hasn't been defined.\n",
            "\t\t - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).\n",
            "\t\t - The return types of methods in your clients/strategies might be incorrect.\n",
            "\t > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.\n",
            "\t > All the actors in your pool crashed. This could be because: \n",
            "\t\t - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 0} is not enough for your run). Use fewer concurrent actors. \n",
            "\t\t - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 0}.\n",
            "Take a look at the Flower simulation examples for guidance <https://flower.ai/docs/framework/how-to-run-simulations.html>.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Simulation crashed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/simulation/legacy_app.py\u001b[0m in \u001b[0;36mstart_simulation\u001b[0;34m(client_fn, num_clients, clients_ids, client_resources, server, config, strategy, client_manager, ray_init_args, keep_initialised, actor_type, actor_kwargs, actor_scheduling)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         hist = run_fl(\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0mserver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitialized_server\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/server/server.py\u001b[0m in \u001b[0;36mrun_fl\u001b[0;34m(server, config)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;34m\"\"\"Train a model on the given server and return the History object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m     hist, elapsed_time = server.fit(\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0mnum_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/server/server.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, num_rounds, timeout)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Starting evaluation of initial global parameters\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/server/strategy/fedavg.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, server_round, parameters)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mparameters_ndarrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters_to_ndarrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0meval_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_ndarrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval_res\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-02c4b298bd5d>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(server_round, parameters, config)\u001b[0m\n\u001b[1;32m     33\u001b[0m     ):\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update model with the latest parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MLPClassifier' object has no attribute 'set_weights'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-b786da1cbbbe>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Start simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     history = fl.simulation.start_simulation(\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mclient_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_client_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mnum_clients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLIENTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/simulation/legacy_app.py\u001b[0m in \u001b[0;36mstart_simulation\u001b[0;34m(client_fn, num_clients, clients_ids, client_resources, server, config, strategy, client_manager, ray_init_args, keep_initialised, actor_type, actor_kwargs, actor_scheduling)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mclient_resources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         )\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Simulation crashed.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Simulation crashed."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=18958)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=18958)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=18958)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=18958)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=18958)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=18958)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test model with Distributed Training"
      ],
      "metadata": {
        "id": "hXO-LCzz15JX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data for Testing - Vẫn con Test data còn lưu lại.\n",
        "test_df= pd.read_csv(\"./test_data.csv\")"
      ],
      "metadata": {
        "id": "ZcUl9SXH2HTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2b0b553-3920-4f3b-cabc-ae77b4876b32"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=9033)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=9033)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=9033)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=9033)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=9033)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=9033)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FOR BINARY"
      ],
      "metadata": {
        "id": "dQSYNL9GSHa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "for _ in range(2):\n",
        "\n",
        "    output_model = f\"./models/i_medianFedAvg_{_}.keras\"\n",
        "    print('--------------------Model thứ: ', _ )\n",
        "    model = load_model(output_model)\n",
        "    s = model_report_list(model, test_data.copy())\n",
        "    #s = model_report_list_multiclass(model, test_data.copy())\n",
        "\n",
        "    result.append(s)\n",
        "    del model"
      ],
      "metadata": {
        "id": "jaraDK_xSTK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc, average_precision_score, f1_score, precision_score, recall_score\n",
        "def model_report_list(model, df_test):\n",
        "    result = []\n",
        "\n",
        "    x_test = df_test.iloc[:, :-1]\n",
        "    y_test = df_test[\"Label\"]\n",
        "\n",
        "    # Get model predictions for the test data\n",
        "    y_pred_prob = model.predict(x_test, verbose=0)\n",
        "\n",
        "    optimal_threshold = 0.5\n",
        "\n",
        "    y_pred = (y_pred_prob > optimal_threshold).astype(\"int32\")\n",
        "\n",
        "    df_test[\"PLabel\"] = y_pred\n",
        "\n",
        "    # Calculate F1 Score\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    result.append(('F1 Score', f1))\n",
        "\n",
        "    # Calculate AUC\n",
        "    auc_score = roc_auc_score(y_test, y_pred_prob)\n",
        "    result.append(('AUC', auc_score))\n",
        "\n",
        "    # Calculate Average Precision\n",
        "    avg_precision = average_precision_score(y_test, y_pred_prob)\n",
        "    result.append(('Average Precision', avg_precision))\n",
        "\n",
        "    # Calculate Precision\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    result.append(('Precision', precision))\n",
        "\n",
        "    # Calculate Recall\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    result.append(('Recall', recall))\n",
        "\n",
        "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    FP = confusion_mat[0][1]\n",
        "    FN = confusion_mat[1][0]\n",
        "\n",
        "    TP = confusion_mat[1][1]\n",
        "    TN = confusion_mat[0][0]\n",
        "\n",
        "    FPR = FP / (FP + TN) if (FP + TN) > 0 else 0  # Avoid division by zero\n",
        "    FNR = FN / (FN + TP) if (FN + TP) > 0 else 0  # Avoid division by zero\n",
        "\n",
        "    result.append(('False Positive Rate', FPR))\n",
        "    result.append(('False Negative Rate', FNR))\n",
        "\n",
        "    del model\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "SuXlWsjE18YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {}\n",
        "\n",
        "# Iterate through each tuple in the first list to use as keys and create lists for their values\n",
        "for metric, value in result[0]:\n",
        "    data[metric] = []\n",
        "\n",
        "# Fill the lists with values from each corresponding position in the remaining lists\n",
        "for res in result:\n",
        "    for i, (metric, value) in enumerate(res):\n",
        "        data[metric].append(value)\n",
        "\n",
        "# Convert the dictionary into a DataFrame\n",
        "result_df = pd.DataFrame(data)\n",
        "\n",
        "print('MEAN_AVG')\n",
        "print(\"-\"*100)\n",
        "print(result_df.mean(axis=0))\n",
        "print(\"-\"*100)"
      ],
      "metadata": {
        "id": "_5ewd_lbPwqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MULTI_CLASSES"
      ],
      "metadata": {
        "id": "Ccl0yIy4Pxrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def model_report_list_multiclass(model, df_test):\n",
        "    result = []\n",
        "\n",
        "    x_test = df_test.iloc[:, :-1]\n",
        "    y_test = df_test[\"Label\"]\n",
        "\n",
        "    # Get model predictions for the test data\n",
        "    y_pred_prob = model.predict(x_test, verbose=0)\n",
        "\n",
        "    # Get predicted labels for multiclass\n",
        "    y_pred = y_pred_prob.argmax(axis=1)\n",
        "\n",
        "    df_test[\"PLabel\"] = y_pred\n",
        "\n",
        "    mapped_y_pred = [encoded_to_original[label] for label in y_pred]\n",
        "    mapped_y_test = [encoded_to_original[label] for label in y_test]\n",
        "\n",
        "    # Generate classification report\n",
        "    class_report = classification_report(mapped_y_test, mapped_y_pred, output_dict=True)\n",
        "    #print(class_report)\n",
        "    result.append(('Classification Report', class_report))\n",
        "\n",
        "    # # Calculate confusion matrix\n",
        "    # confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "    # result.append(('Confusion Matrix', confusion_mat))\n",
        "\n",
        "    del model\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "-r4H6XVPAVja"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report-Multiclass"
      ],
      "metadata": {
        "id": "ZzpsMA3FAxTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "for _ in range(2):\n",
        "\n",
        "    output_model = f\"./models/i_medianFedAvg_{_}.keras\"\n",
        "    print('--------------------Model thứ: ', _ )\n",
        "    model = load_model(output_model)\n",
        "    #s = model_report_list(model, test_data.copy())\n",
        "    s = model_report_list_multiclass(model, test_data.copy())\n",
        "\n",
        "    result.append(s)\n",
        "    del model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHzQeYwN2Cau",
        "outputId": "8c87647b-d978-47c3-bd5d-2baf33d0c0ec"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------Model thứ:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'nadam', because it has 15 variables whereas the saved optimizer has 2 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'nadam', because it has 15 variables whereas the saved optimizer has 2 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------Model thứ:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Tạo một DataFrame để lưu các giá trị trung bình\n",
        "df_avg_classes = pd.DataFrame(columns=['precision', 'recall', 'f1-score', 'support', 'accuracy'])\n",
        "\n",
        "# Lấy các class từ báo cáo đầu tiên\n",
        "classes = list(result[0][0][1].keys())  # result[0][0][1] chứa dictionary của classification report\n",
        "\n",
        "# Xóa các chỉ số tổng hợp (macro avg, weighted avg, accuracy, support)\n",
        "classes = [cls for cls in classes if cls not in ['accuracy', 'macro avg', 'weighted avg']]\n",
        "\n",
        "# Tính trung bình cho từng class và lấy support từ báo cáo đầu tiên\n",
        "for cls in classes:\n",
        "    precision_avg = 0\n",
        "    recall_avg = 0\n",
        "    f1_avg = 0\n",
        "    # Lặp qua các báo cáo trong result để tính trung bình precision, recall, f1-score\n",
        "    for report in result:\n",
        "        precision_avg += report[0][1][cls]['precision']\n",
        "        recall_avg += report[0][1][cls]['recall']\n",
        "        f1_avg += report[0][1][cls]['f1-score']\n",
        "\n",
        "    # Tính trung bình\n",
        "    num_reports = len(result)\n",
        "\n",
        "    # Lấy support từ báo cáo đầu tiên\n",
        "    support = result[0][0][1][cls]['support']\n",
        "\n",
        "    df_avg_classes.loc[cls] = [\n",
        "        precision_avg / num_reports,\n",
        "        recall_avg / num_reports,\n",
        "        f1_avg / num_reports,\n",
        "        support,  # Thêm support vào DataFrame\n",
        "        None  # Sẽ tính accuracy sau\n",
        "    ]\n",
        "\n",
        "# Tính trung bình cho các chỉ số tổng hợp (macro avg và weighted avg)\n",
        "for avg_type in ['macro avg', 'weighted avg']:\n",
        "    precision_avg = 0\n",
        "    recall_avg = 0\n",
        "    f1_avg = 0\n",
        "    for report in result:\n",
        "        precision_avg += report[0][1][avg_type]['precision']\n",
        "        recall_avg += report[0][1][avg_type]['recall']\n",
        "        f1_avg += report[0][1][avg_type]['f1-score']\n",
        "\n",
        "    # Sử dụng support từ 'macro avg' trong báo cáo đầu tiên\n",
        "    support = result[0][0][1][avg_type]['support'] if avg_type in result[0][0][1] and 'support' in result[0][0][1][avg_type] else None\n",
        "\n",
        "    df_avg_classes.loc[avg_type] = [\n",
        "        precision_avg / num_reports,\n",
        "        recall_avg / num_reports,\n",
        "        f1_avg / num_reports,\n",
        "        support,  # Thêm support nếu có\n",
        "        None  # Sẽ tính accuracy sau\n",
        "    ]\n",
        "\n",
        "# Tính trung bình accuracy cho tất cả các mô hình\n",
        "accuracy_avg = 0\n",
        "for report in result:\n",
        "    accuracy_avg += report[0][1]['accuracy']\n",
        "\n",
        "df_avg_classes['accuracy'] = accuracy_avg / num_reports\n",
        "\n",
        "# In kết quả trung bình\n",
        "print(df_avg_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Khp3Ty_LNw64",
        "outputId": "f980d04a-66de-4e94-a282-0cd6840508bd"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                precision    recall  f1-score  support  accuracy\n",
            "Analysis         0.000000  0.000000  0.000000     78.0   0.90616\n",
            "Backdoor         0.000000  0.000000  0.000000     92.0   0.90616\n",
            "Benign           0.999185  0.972774  0.985802  71842.0   0.90616\n",
            "DoS              0.954545  0.006187  0.012235    889.0   0.90616\n",
            "Exploits         0.695126  0.688016  0.690889   6058.0   0.90616\n",
            "Fuzzers          0.474881  0.841162  0.606743   5940.0   0.90616\n",
            "Generic          0.808716  0.091790  0.164782    877.0   0.90616\n",
            "Reconnaissance   0.680531  0.606782  0.641178   3362.0   0.90616\n",
            "Shellcode        0.000000  0.000000  0.000000    401.0   0.90616\n",
            "Worms            0.000000  0.000000  0.000000     44.0   0.90616\n",
            "macro avg        0.461298  0.320671  0.310163  89583.0   0.90616\n",
            "weighted avg     0.922732  0.906160  0.903324  89583.0   0.90616\n"
          ]
        }
      ]
    }
  ]
}